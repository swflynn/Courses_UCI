\documentclass{article}
%=============================================================================80
%	                          Packages                                     %
%==============================================================================%
% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{float}
\usepackage{subcaption}
\usepackage[margin=0.7in]{geometry}
\usepackage[version=4]{mhchem}
%==============================================================================%
%                           User-Defined Commands                              %
%==============================================================================%
% User-Defined Commands
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\benum}{\begin{enumerate}}
\newcommand{\eenum}{\end{enumerate}}
\newcommand{\pd}{\partial}
\newcommand{\dg}{\dagger}
%==============================================================================%
%                             Title Information                                %
%==============================================================================%
\title{Chem237: Lecture 1}
\date{4/3/18}
\author{Alan Robledo, Shane Flynn}
%==============================================================================%
\begin{document}

\maketitle
We have talked about linear transformations that consist of taking some general matrix \textbf{A} $\in \mathbb{C}$ and having it act on a vector $\vec{x}$ to create a new vector $\vec{b}$, i.e.
\be
  \textbf{A} \vec{x} = \vec{b}
\ee
In situations where the matrix multiplication results the same vector and a scalar, we call it an eigenvalue problem,
\be \label{eq:eigen_prob}
  \textbf{A} \vec{x} = \lambda \vec{x} .
\ee
In this case, we call $\vec{x}$ an eigenvector of \textbf{A}, and $\lambda$ is an eigenvalue of \textbf{A}.
The scalar $\lambda$ is only an eigenvalue of \textbf{A} if $\textbf{A} - \lambda \textbf{I}$ is singular.
This idea can be seen more clearly by manipulating equation (\ref{eq:eigen_prob}).
\be
  \textbf{A} \vec{x} = \lambda \vec{x} \quad \Longrightarrow \quad \textbf{A} \vec{x} - \lambda \vec{x} = \textbf{0} \quad \Longrightarrow \quad (\textbf{A} - \lambda \textbf{I})\vec{x} = \textbf{0}
\ee
Notice how we multiplied $\lambda$ by the identity matrix because we cannot subtract a scalar from a matrix.
If we were to mutliply both sides by $(\textbf{A} - \lambda \textbf{I})^{-1}$, then we would be left with the trivial solution that all $x_i = 0$.
So what we'll do is impose the idea that $(\textbf{A} - \lambda \textbf{I})$ does not have an inverse.
This means that the determinant of $(\textbf{A} - \lambda \textbf{I})$ must be equal to 0.
Recall that noninvertable or \textbf{singular} matrices have a determinant equal to 0.
This allows us to write
\be
  \text{det}(\textbf{A} - \lambda \textbf{I}) = 0
\ee
and is called the \textbf{characteristic equation}.
The characteristic equation is a polynomial of degree N where N is the dimensionality of \textbf{A}.
How do we know there will always be N eigenvalues?
Let's consider a matrix \textbf{A} $\in \mathbb{R}^3$, and write out the determinant explicitly
\be
  \text{det}
  \begin{bmatrix}
    A_{11} - \lambda  & A_{12} & A_{13} \\
    A_{21} & A_{22} - \lambda & A_{23} \\
    A_{31} & A_{32} & A_{33} - \lambda \\
  \end{bmatrix}
\ee
so we can make use of the technique called \textbf{determinant expansion by minors}.
We can pick any row or column for this technique and we'll choose to go along the first row.
Therefore, the expansion becomes,
\be
  \text{det}(\textbf{A} - \lambda \textbf{I}) = (A_{11} - \lambda) \text{det}
  \begin{bmatrix}
    A_{22} - \lambda & A_{23} \\
    A_{32} & A_{33} - \lambda \\
  \end{bmatrix}
  - (A_{12}) \text{det}
  \begin{bmatrix}
    A_{21} & A_{23} \\
    A_{31} & A_{33} - \lambda \\
  \end{bmatrix}
  + (A_{13}) \text{det}
  \begin{bmatrix}
    A_{21} & A_{22} - \lambda \\
    A_{31} & A_{32} \\
  \end{bmatrix}
\ee
When we write out the remaining three determinants,
\be
\begin{split}
  \text{det}(\textbf{A} - \lambda \textbf{I}) &= (A_{11} - \lambda)\Big[(A_{22} - \lambda)(A_{33} - \lambda) - (A_{32})(A_{23})\Big] - (A_{12}) \Big[(A_{21})(A_{33} - \lambda) - (A_{31})(A_{23}) \Big] \\
  & + (A_{13}) \Big[ (A_{21})(A_{32}) - (A_31)(A_{22} - \lambda) \Big] = 0\\
\end{split}
\ee
we can see that the result is a polynomial of degree 3 in $\lambda$ since each matrix element is a scalar.
This same technique can be used to show that an NxN matrix can yield a characteristic equation with N roots and thus N complex eigenvalues (assuming that the matrix itself is complex).
The technique for finding the ith eigenvector corresponding to the ith eigenvalue is to solve equation 3 by plugging in $\lambda_i$ and solving for $\vec{x}$.
Once we have found the eigenvalues and eigenvectors of \textbf{A}, we can write the eigendecomposition of \textbf{A}; a very useful technique that is used a lot in chemistry and physics.
\be \label{eq:eigendecomposition}
  \textbf{A} = \textbf{X} \Lambda \textbf{X}^{-1}
\ee
Here, \textbf{X} is an NxN matrix where each column is an eigenvector of \textbf{A},
\be
  \textbf{X} =
  \begin{bmatrix}
    \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_N
  \end{bmatrix}
\ee
and $\Lambda$ is a diagonal matrix where the ith diagonal element corresponds to the eigenvector in the ith column of \textbf{X}.
\be
  \Lambda =
  \begin{bmatrix}
    \lambda_1 & \cdots & 0 \\
    \vdots & & \vdots \\
    0 & \cdots & \lambda_N \\
  \end{bmatrix}
\ee
You will often find people in literature introduce eigendecomposition as finding a matrix \textbf{X} that diagonalizes \textbf{A}.
This is beause any matrix written in the basis of its eigenvectors results in a diagonal matrix of its eigenvalues.
To see this, we can left multiply equation (\ref{eq:eigendecomposition}) by $\textbf{X}^{-1}$ and right multiply by \textbf{X} to get \textbf{A} in the basis of its eigenvectors.
\be
  \Lambda = \textbf{X}^{-1} \textbf{A} \textbf{X}
\ee

A special case to consider is when our matrix \textbf{A} is hermitian.
In this case, $\textbf{A} = \textbf{A}^{\dagger}$ where $\dagger$ denotes the conjugate transpose.
The eigenvalues of any hermitian matrix are real.
To prove this, consider equation (\ref{eq:eigen_prob}) for a hermitian matrix \textbf{A}.
\be
  \textbf{A} \vec{x} = \lambda \vec{x}
\ee
Left multiplying by $\vec{x}^{\dagger}$ gives
\be
  \begin{split}
    \vec{x}^{\dagger} \textbf{A} \vec{x} &= \vec{x}^{\dagger} \lambda \vec{x} \\
    &= \lambda \vec{x}^{\dagger} \vec{x} \\
    &= \lambda |\vec{x}|^2 \\
  \end{split}
\ee
where $|\vec{x}|$ denotes the modulous of $\vec{x}$.
Then we take the conjugate transpose of both sides
\be
  \begin{split}
    (\vec{x}^{\dagger} \textbf{A} \vec{x})^{\dagger} &= (\lambda |\vec{x}|^2)^{\dagger} \\
    \vec{x}^{\dagger} \textbf{A}^{\dagger} (\vec{x}^{\dagger})^{\dagger} &= \lambda^{\dagger} (|\vec{x}|^2)^{\dagger} \\
  \end{split}
\ee
Since \textbf{A} is hermitian, then $\textbf{A}^{\dagger} = \textbf{A}$.
Also, $(|\vec{x}|^2)^{\dagger} = |\vec{x}|^2$ because the square modulous of any complex number is a real number.
So,
\be
  \begin{split}
    \vec{x}^{\dagger} \textbf{A}^{\dagger} (\vec{x}^{\dagger})^{\dagger} &= \lambda^{\dagger} (|\vec{x}|^2)^{\dagger} \\
    \vec{x}^{\dagger} \textbf{A} \vec{x} &= \lambda^{\dagger} |\vec{x}|^2 \\
    \vec{x}^{\dagger} \lambda \vec{x} &= \lambda^{\dagger} \\
    \lambda |\vec{x}|^2 &= \lambda^{\dagger} |\vec{x}|^2 \\
    \lambda &= \lambda^{\dagger} \\
    \lambda &= \lambda^*
  \end{split}
\ee
The last line follows from the fact that the transpose of any scalar is the scalar itself.
Another important property of hermitian matrices is that each of the eigenvectors are orthogonal.
To prove this, consider two distinct eigenvectors and eigenvalues of a hermitian matrix \textbf{A} such that
\be
  \textbf{A} \vec{x}_1 = \lambda_1 \vec{x}_1 \quad \quad \textbf{A} \vec{x}_2 = \lambda_2 \vec{x}_2
\ee
If we mutliply the first equation by $\vec{x}_2^{\dagger}$ and take the conjugate transpose of both sides then
\be
  \begin{split}
    \vec{x}_2^{\dagger} \textbf{A} \vec{x}_1 &= \vec{x}_2^{\dagger} \lambda_1  \vec{x}_1 \\
    (\vec{x}_2^{\dagger} \textbf{A} \vec{x}_1)^{\dagger} &= (\vec{x}_2^{\dagger} \lambda_1  \vec{x}_1)^{\dagger} \\
    \vec{x}_1^{\dagger} \textbf{A}^{\dagger} \vec{x}_2 &= \vec{x}_1^{\dagger} \lambda_1^{\dagger} \vec{x}_2 \\
    \vec{x}_1^{\dagger} \textbf{A} \vec{x}_2 &= \vec{x}_1^{\dagger} \lambda_1 \vec{x}_2 \\
    \vec{x}_1^{\dagger} \lambda_2 \vec{x}_2 &= \vec{x}_1^{\dagger} \lambda_1 \vec{x}_2 \\
    \lambda_2 \vec{x}_1^{\dagger} \vec{x}_2 &= \lambda_1 \vec{x}_1^{\dagger} \vec{x}_2
  \end{split}
\ee
which is only possible if the eigenvectors are orthogonal, i.e. $\vec{x}_1^{\dagger} \vec{x}_2 = 0$ because $\lambda_1 \neq \lambda_2$.
If we refer back to the eigendecomposition of a hermitian matrix \textbf{A}, we can say then that $\Lambda^{\dagger} = \Lambda$ because the eigenvalue matrix is diagonal and each of the eigenvalues are real.
We can also say that \textbf{X} is unitary, i.e. $\textbf{X}^{-1} = \textbf{X}^{\dagger}$, because \textbf{X} is made up of the eigenvectors of \textbf{A}, which are all orthogonal.

The fact that the eigenvectors of a hermitian matrix are orthogonal allows us to write the eigenvalue problem as equation (\ref{eq:eigen_prob}), which is the form that we are all used to seeing.
For any general matrix \textbf{A}, we can write the generalized eigenvalue problem as
\be
  (\textbf{A} - \lambda \textbf{S}) \vec{x} = \vec{0}
\ee
where \textbf{S} is the overlap matrix whose off-diagonal elements are non-zero.  

\end{document}
